{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb70f97-325c-42c9-943c-a7a487920f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcello-deluca/.local/share/uv/python/cpython-3.11.9-macos-aarch64-none/lib/python3.11/xml/etree/ElementTree.py:1655: RuntimeWarning: coroutine 'generate_responses' was never awaited\n",
      "  attrib = {}\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 50378/50378 [00:00<00:00, 5950622.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found  100  prompts to feed to LLM API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/cs/prc5nc1d48xd7bwtsg4_wnqc0000gn/T/ipykernel_65133/1816862697.py:111: RuntimeWarning: coroutine 'async_generate' was never awaited\n",
      "  get_responses = [async_generate(prompt, my_project) for prompt in prompts]\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot run the event loop while another loop is running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 113\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompts), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m prompts to feed to LLM API\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m get_responses \u001b[38;5;241m=\u001b[39m [async_generate(prompt, my_project) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 113\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mrun_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m structuredLists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mget_responses)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#display_responses(prompts, responses)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 67\u001b[0m, in \u001b[0;36mrun_async\u001b[0;34m(coroutine)\u001b[0m\n\u001b[1;32m     65\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[1;32m     66\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.9-macos-aarch64-none/lib/python3.11/asyncio/base_events.py:630\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    633\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.9-macos-aarch64-none/lib/python3.11/asyncio/base_events.py:591\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot run the event loop while another loop is running"
     ]
    }
   ],
   "source": [
    "#contraindications_to_diseases_v2\n",
    "\n",
    "import asyncio\n",
    "from typing import List\n",
    "from vertexai.generative_models import GenerativeModel, Part, SafetySetting, FinishReason\n",
    "from google.cloud import aiplatform\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, wait_random_exponential\n",
    "\n",
    "# Initialize Google Cloud project and location\n",
    "PROJECT_ID = \"mtrx-wg2-modeling-dev-9yj\"\n",
    "my_project = PROJECT_ID\n",
    "LOCATION = \"us-central1\"\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Initialize the Gemini model\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    ),\n",
    "]\n",
    "\n",
    "async def generate_content(prompt: str) -> str:\n",
    "    response = await model.generate_content_async(prompt)\n",
    "    return response.text\n",
    "\n",
    "async def process_responses(responses: List[asyncio.Task]) -> List[str]:\n",
    "    results = []\n",
    "    for response in responses:\n",
    "        result = await response\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "async def generate_responses(prompts: List[str]) -> List[str]:\n",
    "    tasks = [asyncio.create_task(generate_content(prompt)) for prompt in prompts]\n",
    "    responses = await process_responses(tasks)\n",
    "    return responses\n",
    "\n",
    "def run_async(coroutine):\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    return loop.run_until_complete(coroutine)\n",
    "\n",
    "def display_responses(prompts: List[str], responses: List[str]):\n",
    "    html = \"<table><tr><th>Prompt</th><th>Response</th></tr>\"\n",
    "    for prompt, response in zip(prompts, responses):\n",
    "        html += f\"<tr><td>{prompt}</td><td>{response}</td></tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def get_input_text(active_ingredient_data, contraindication_text):\n",
    "    text = \"Produce a list of diseases contraindicated for the active ingredient \" + str(active_ingredient_data) + \" in the following contraindications list:\\n\" + str(contraindication_text) + \"Please format the list as [\\'item1\\', \\'item2\\', ... ,\\'itemN\\']. Do not include any other text in the response. If no diseases are contraindicated for, return an empty list as \\'[]\\'. If the drug is only used for diagnostic purposes, return \\'diagnostic/contrast/radiolabel\\'. Do not include hypersensitivity or allergy to the named drug as a contraindication. This code is being deployed in bulk so if the contraindications section is just \\<template\\> or similar, return an empty list. Be mindful of the distinction between contraindications in patient groups and \"\n",
    "    return text   \n",
    "\n",
    "def generate_prompts(contraindications_data, active_ingredients_data, limit) -> list[str]:\n",
    "    print(\"generating prompts...\")\n",
    "    prompts = []\n",
    "    n_contraindications = len(contraindications_data)\n",
    "    for index, item in tqdm(enumerate(contraindications_data), total=n_contraindications):\n",
    "        if index < limit:\n",
    "            prompts.append(get_input_text(active_ingredients_data[index], item))\n",
    "    return prompts\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=120))\n",
    "async def async_generate(prompt, my_project):\n",
    "  vertexai.init(project=my_project, location=\"us-central1\")\n",
    "  model = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\",\n",
    "  )\n",
    "  response = await model.generate_content_async(\n",
    "      [prompt],\n",
    "      generation_config=generation_config,\n",
    "      safety_settings=safety_settings,\n",
    "      stream=False,\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "drugs_to_contraindications = pd.read_excel(\"../contraindicationList.xlsx\")\n",
    "contraindications_data = list(drugs_to_contraindications['contraindications'])\n",
    "active_ingredients_data = list(drugs_to_contraindications['active ingredient'])\n",
    "\n",
    "prompts = generate_prompts(contraindications_data, active_ingredients_data, limit=100)\n",
    "print(\"found \", len(prompts), \" prompts to feed to LLM API\")\n",
    "\n",
    "get_responses = [async_generate(prompt, my_project) for prompt in prompts]\n",
    "\n",
    "responses = await run_async(generate_responses(prompts))\n",
    "structuredLists = await asyncio.gather(*get_responses)\n",
    "\n",
    "#display_responses(prompts, responses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f25c1-0802-4a23-af8a-82a7e1f55f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
